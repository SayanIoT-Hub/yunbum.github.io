I"!<p>모바일 로봇에 장착하거나 연결해서 사용할 수 있는 다양한 악세사리를 제공하고 있습니다.</p>
<center><img src="./assets/img/posts/20210228/ML_cloud.jpg" width="480px" /></center>
<p>Let me try to explain; I am in the process of immersing myself into the world of Machine Learnin.</p>

<p>Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.</p>

<center><img src="./assets/img/posts/20210228/nnet_flow.gif" /></center>

<p>The library started very narrowly, with just the following functionality:</p>
<ul>
  <li><strong>create</strong> a neural network based on the following parameters:
    <ul>
      <li>number of inputs</li>
      <li>size and number of hidden layers</li>
      <li>number of outputs</li>
      <li>learning rate</li>
    </ul>
  </li>
  <li><strong>forward propagate</strong> or predict the output values when given some inputs</li>
  <li><strong>learn</strong> through back propagation using gradient descent</li>
</ul>

<p>I restricted the model to be sequential, the only activation function I implemented was sigmoid:</p>

<center><img src="./assets/img/posts/20210228/nn_diagram.png" /></center>

<p>With my neural network coded, I tested it with a very basic problem, the famous XOR problem.</p>

<p>XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:</p>

<center><img src="./assets/img/posts/20210228/xor_problem.png" /></center>

<p>As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true.</p>

<p>For the test I created a neural network with my library:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">Neural_Network</span> <span class="k">as</span> <span class="n">nn</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.03</span>

<span class="n">NN</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden_layers</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div>

<p>Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">training_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">learning_rounds</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rnd</span><span class="p">.</span><span class="n">random</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">rnd</span><span class="p">.</span><span class="n">random</span><span class="p">()</span>
    <span class="n">training_data</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="mi">0</span> <span class="k">if</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">)</span> <span class="k">else</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<p>The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="n">canvas</span><span class="p">.</span><span class="n">set_window_title</span><span class="p">(</span><span class="s">'Learning XOR Algorithm'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="n">axs1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">'3d'</span><span class="p">)</span>
<span class="n">axs2</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:</p>

<p>As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">axs1</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span>
                  <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">cmap</span><span class="o">=</span><span class="s">'viridis'</span><span class="p">,</span>
                  <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                  <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                  <span class="n">antialiased</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>The end result looks something like this:</p>
<center><img src="./assets/img/posts/20210228/Surface_XOR.jpg" /></center>

<p>Then we reshape the z array as a one dimensional array to use it to color the scatter plot:</p>

<center><img src="./assets/img/posts/20210228/Final_XOR_Plot.jpg" /></center>

<p>So my baby ML library is completed for now, but still I would like to enhance it in several ways:</p>

<ul>
  <li>include multiple activation functions (ReLu, linear, Tanh, etc.)</li>
  <li>allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.)</li>
  <li>have batch and epoch training schedules functionality</li>
  <li>save and load trained model to file</li>
</ul>

<p>I will get to it soon…</p>
:ET