I"ó<p>As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.</p>

<h2 id="what-is-a-perceptron">What is a perceptron?</h2>

<p>A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.</p>

<p>Back in the late 1950‚Äôs, a young <a href="https://en.wikipedia.org/wiki/Frank_Rosenblatt">Frank Rosenblatt</a> devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.</p>

<p>In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.</p>

<center><img src="./assets/img/posts/20210125/Perceptron.png" /></center>

<p>Imagine you have a ‚Äòneuron‚Äô and to ‚Äòactivate‚Äô it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.</p>

<p>In more mathematical terms, a perceptron is an array of values (let‚Äôs call them weights), and the rules to apply such values to an input signal.</p>

<p>For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \; x_2 = 2\; and \; x_3 = 3$, if it‚Äôs weights are $w_1 = 0.5,\; w_2 = 1\; and \; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.</p>

<p style="text-align:center">\(<br />
\begin{align}
\begin{split}
\left(x_1 * w_1\right) + \left(x_2 * w_2\right) + \left(x_3 * w_3\right)
\end{split}
\end{align}
\)</p>

<p style="text-align:center">\(<br />
\begin{align}<br />
\begin{split}<br />
\left(0.5 * 1\right) + \left(1 * 2\right) + \left(-1 * 3\right) = 0.5 + 2 - 3 = -0.5
\end{split}<br />
\end{align}<br />
\)</p>

<p>Typically when this value is obtained, we need to apply an ‚Äúactivation‚Äù function to smooth the output.</p>

<p>For this we need a set of data that it is already classified, we call this a training set.</p>

<p>The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">This video series by 3 blue 1 brown explains it wonderfuly.</a></p>

<p>The neuron has 3 inputs and weights to calculate its output:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>input 1 is the X coordinate of the point,
Input 2 is the y coordinate of the point,
Input 3 is the bias and it is always 1

Input 3 or the bias is required for lines that do not cross the origin (0,0)
</code></pre></div></div>

<p>The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.</p>

<p>The output of the perceptron is calculated with the following activation function:
    if x * weight_x + y weight_y + weight_bias is positive then 1 else 0</p>

<p>The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Expected</th>
      <th style="text-align: center">Calculated</th>
      <th style="text-align: center">Error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">1</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">-1</td>
      <td style="text-align: center">1</td>
      <td style="text-align: center">-1</td>
    </tr>
  </tbody>
</table>

<p>With every point that is learned if the error is not 0 the weights are adjusted according to:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>New_weight = Old_weight + error * input * learning_rate
for example: New_weight_x = Old_weight_x + error * x * learning rate
</code></pre></div></div>

<p>In this particular case, I coded the learning_rate to decrease with every iteration as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>learning_rate = 0.01 / (iteration + 1)
</code></pre></div></div>

<p>this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.</p>

<center><img src="./assets/img/posts/20210125/Learning_1000_points_per_iteration.jpg" /></center>

<p>In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.</p>
:ET