<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://github.com/yunbum/yunbum.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://github.com/yunbum/yunbum.github.io/" rel="alternate" type="text/html" /><updated>2021-07-11T07:53:36+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/feed.xml</id><title type="html">The Minimum Viable Model</title><subtitle>Artificial Intelligence trends and concepts made easy.</subtitle><author><name>Ybbaek</name></author><entry><title type="html">RLmodel 모바일로봇 개발이력 / Mobile Robot History!</title><link href="https://github.com/yunbum/yunbum.github.io/AI-and-intellectual-property.html" rel="alternate" type="text/html" title="RLmodel 모바일로봇 개발이력 / Mobile Robot History!" /><published>2021-04-20T00:00:00+09:00</published><updated>2021-04-20T00:00:00+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/AI-and-intellectual-property</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/AI-and-intellectual-property.html">&lt;h2 id=&quot;모바일로봇-개발-history&quot;&gt;모바일로봇 개발 History&lt;/h2&gt;
&lt;p&gt;초기 중고 RC가, KINEX 블록 등으로 개발 프로토 타입을 시작으로 개조,변형,튜닝등의 다양한 테스트와 보완을 거처 최종 수십 km 자율주행 완료하여 SRC모델에 완성에 이르기까지 상당히 다양한 HW,SW 보완을 진행했고 현재도 진행중 입니다.&lt;/p&gt;

&lt;p&gt;자율주행 모바일 로봇개발 목적으로 개발한 플랫폼 &lt;a href=&quot;https://github.com/yunbum/SRC/&quot;&gt;check github repo&lt;/a&gt; .&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/qJrNXtsEzZo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;초기 개발버전 (Legacy SRC Mobile robots) 테스트 차량들&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210420/src_history.jpg&quot; alt=&quot;SRC history&quot; /&gt;
&lt;small&gt;[SRC history] 여러종류의 RC 차량의 개조와 테스트를 거처, 모든 구성 부품들이 완전히 개조되고 보완되어 업그레이드 되어 개발되었습니다..&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;SRC 로봇은 약 소형 RC 자동차 개량으로 출발해서, 다양한 RC 차량을 별도의 모터드라이버, 아두이노, 라즈베리파이, 카메라, 라이다 등을 부착하여 튜닝하는 것을 시작으로, GPS, Lidar 등을 추가하고 Waypoint 제어등을 거쳐 현재의 완벽한 자율주행 차량으로 업그레이드 되게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210420/metal_frame.jpg&quot; alt=&quot;Metal frame&quot; /&gt;
&lt;small&gt;[Metal frame] 전체 메탈 프레임 초기 모델중 하나.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;다양한 프레임 소재 및 차량 종류의 테스트를 거처 최종으로 전체 메탈프레임 구조를 갖추고 완성이 되게 되어 매우 강한 강성과 함께 파손우려가 없고 수리, 보완에도 용이하도록 개발이 완료되었습니다.&lt;/p&gt;

&lt;p&gt;So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210420/src-b2-back1.jpg&quot; alt=&quot;The Next SRC&quot; /&gt;
&lt;small&gt;&lt;a href=&quot;https://github.com/yunbum/SRC&quot;&gt;The Next SRC&lt;/a&gt; 는 보다 강력한 모터, 주행알고리즘 등으로 개량되어, 커리큘럼 및 예제와 함께 (Python, LabVIEW 등) 모바일 버전으로도 준비중에 있습니다. .&lt;/small&gt;&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="opinion" /><category term="copyright" /><category term="creativity" /><category term="neural networks" /><category term="machine learning" /><category term="artificial intelligence" /><summary type="html">모바일로봇 개발 History 초기 중고 RC가, KINEX 블록 등으로 개발 프로토 타입을 시작으로 개조,변형,튜닝등의 다양한 테스트와 보완을 거처 최종 수십 km 자율주행 완료하여 SRC모델에 완성에 이르기까지 상당히 다양한 HW,SW 보완을 진행했고 현재도 진행중 입니다.</summary></entry><entry><title type="html">자율주행 플랫폼 SRC 란, what is SRC (Self Driving Rc Car?</title><link href="https://github.com/yunbum/yunbum.github.io/back-to-basics.html" rel="alternate" type="text/html" title="자율주행 플랫폼 SRC 란, what is SRC (Self Driving Rc Car?" /><published>2021-04-02T00:00:00+09:00</published><updated>2021-04-02T00:00:00+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/back-to-basics</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/back-to-basics.html">&lt;p&gt;SRC 은 GPS, Camera, Lidar, IMU, 등의 센서를 기반으로 자율주행 교육 및 주행알고리즘, 센서퓨전 등을 쉽고 효과적으로 개발 할 수 있는 자율주행 모바일로봇 플랫폼 입니다.:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/yunbum/SRC&quot;&gt;SRC 는 자율주행 알고리즘 및 교육목적으로 개발 되었습니다.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cafe.naver.com/iltech&quot;&gt;10여년 간 네이버 카페를 운영하여 관련 예제나 테스트 결과등을 공유하고 있습니다.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCd23NgICe3702uqAAk4HYFQ/videos&quot;&gt;다른 RLmodel 유투브 영상들&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210402/src_hw-sw.png&quot; alt=&quot;HW SW 모듈개발 및 주행테스트&quot; /&gt;
&lt;small&gt;다양한 HW 및 SW 모듈로 주행알고리즘 검증 및 센서퓨전 테스트에 적용완료 &lt;/small&gt;&lt;/p&gt;

&lt;p&gt;차량제어는 Python, C, 등의 컴퓨터언어와도 호환이 되도록 시리얼통신으로 제어할 수 있습니다. LabVIEW 라고하는 National Instrument 사의 프로그래밍 언어도 지원합니다.&lt;/p&gt;

&lt;h1 id=&quot;소개intro&quot;&gt;소개/Intro&lt;/h1&gt;

&lt;h2 id=&quot;모델구분-및-특징&quot;&gt;모델구분 및 특징&lt;/h2&gt;
&lt;p&gt;SRC (&lt;strong&gt;S&lt;/strong&gt;elf driving &lt;strong&gt;R&lt;/strong&gt;emote control &lt;strong&gt;C&lt;/strong&gt;ar) 는 자율주행 차량의 영어 약자 이면 현재 SRC-A,B,C,D 등의 타입이 있습니다.
타입별 차이는 크기, 속도, 모터토크 등으로 구분되어 나누어 집니다.
&lt;img src=&quot;./assets/img/posts/20210402/SRC_models.png&quot; alt=&quot;SRC models&quot; /&gt;
&lt;small&gt;개발시간 전체 약 10여년. 최종 전체 금속기반의 구조는 2019년 완성&lt;/small&gt;&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/OcdVl3k5qS0&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;주요특징&quot;&gt;주요특징&lt;/h2&gt;
&lt;p&gt;프레임은 전체가 알루미늄, 철 등의 금속. 일반 휴대용 보조배터리 파워로 일반 USB 충전이 가능하여 편리하며. 통 고무 타이어로 구성되어 펑크날 위험이 없습니다. (&lt;a href=&quot;https://www.thinkautomation.com/bots-and-ai/a-history-of-automation-the-rise-of-robots-and-ai/&quot;&gt;source&lt;/a&gt;), 구조와 강성이 상당히 개량되어 전체적인 안정성은 일반 플라스틱 RC 카에 비할 수 없고, 현재 수십 km 실외 운행으로 내구성 검증도 완료:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210402/src-b2_3.jpg&quot; alt=&quot;Frame body&quot; /&gt;
&lt;small&gt;개발시간 전체 약 10여년. 최종 전체 금속기반의 구조는 2019년 완성&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;모듈센서&quot;&gt;모듈/센서&lt;/h2&gt;
&lt;p&gt;개별 모듈들의 구성은 맞춤형으로 제공 될 수 있으며, 기본 모터와 모터드라이버, 상위제어기(아두이노) 만으로도 제공이 되며 제어명령은 USB 을 통한 시리얼 통신으로 제어가 가능합니다..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210402/SRC-B_parts.png&quot; alt=&quot;sensors &amp;amp; modules&quot; /&gt;
&lt;small&gt;Full option 상태의 센서 및 기타 모듈 구성도&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;튜닝-업그레이드&quot;&gt;튜닝/ 업그레이드&lt;/h2&gt;
&lt;p&gt;동일 차량모델 (A,B,C,D) 에 속도, 토크를 2배로 향상할 수 있는 듀얼모터 (Dual moter) 옵션으로 선택이 가능하여 등판능력, 속도 등을 높힘 모델로 선택이 가능합니다.&lt;/p&gt;

&lt;center&gt;&lt;img style=&quot;float: left;margin-right: 1em;&quot; src=&quot;./assets/img/posts/20210402/dual_motor.jpg&quot; width=&quot;210&quot; height=&quot;200&quot; /&gt;&lt;/center&gt;
&lt;p&gt;프레임 강성을 유지하기 위해 보완 패치들로 접속부분 추가 체결함
모바일 로봇의 종류는 기본 SRC 차량모델에 이어 ASV 보트(선박) 모델도 있으며 교육목적의 Pendulum, Ball Balance robot 등도 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;강화학습-적용-플랫폼&quot;&gt;강화학습 적용 플랫폼&lt;/h2&gt;

&lt;p&gt;모든 RLmodel 로봇들은 강화학습을 적용하고 테스트 해볼 수 있는 목적으로 개발이 시작되었으며 현재도 다양한 접근으로 예제들을 검토 중입니다.&lt;/p&gt;

&lt;h2 id=&quot;기타-편의사항&quot;&gt;기타 편의사항&lt;/h2&gt;
&lt;p&gt;SRC 차량은 간단한 시리얼 프로토콜 제어가 가능하고 부가 기능으로는 스피커, LED dot matrix, Light 등을 옵션으로 창작하여 테스트 시 차량의 상태나 프로그램의 동작 세부사항 확인이 쉽도록 하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210402/led-light.png&quot; alt=&quot;post7-alexa-steps&quot; /&gt;
&lt;small&gt;야간주행 테스트를 위한 LED light&lt;/small&gt;
추가적으로 ROS 환경에 적합하도록 패키지도 개발 중 입니다.&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="theory" /><category term="Robot" /><category term="RC" /><category term="GPS" /><category term="Camera" /><category term="Lidar" /><summary type="html">SRC 은 GPS, Camera, Lidar, IMU, 등의 센서를 기반으로 자율주행 교육 및 주행알고리즘, 센서퓨전 등을 쉽고 효과적으로 개발 할 수 있는 자율주행 모바일로봇 플랫폼 입니다.: SRC 는 자율주행 알고리즘 및 교육목적으로 개발 되었습니다. 10여년 간 네이버 카페를 운영하여 관련 예제나 테스트 결과등을 공유하고 있습니다. 다른 RLmodel 유투브 영상들</summary></entry><entry><title type="html">ASV 자율운항 보트 / Autonomous Surface Vehicle</title><link href="https://github.com/yunbum/yunbum.github.io/starting-the-adventure.html" rel="alternate" type="text/html" title="ASV 자율운항 보트 / Autonomous Surface Vehicle" /><published>2021-03-24T00:00:00+09:00</published><updated>2021-03-24T00:00:00+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/starting-the-adventure</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/starting-the-adventure.html">&lt;p&gt;자율운항 선박개발 및 알고리즘 테스트 목적으로 RLmodel ASV-A1 을 출시, 자율운항 선박(보트) 등의 주행알고리즘 개발이나 선박주행의 센서퓨전, 기타 제어로직 연구에 적합하도록 개발되었습니다.
사양:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;수중모터(Thruster) 2개의 속도제어를 이용하여 방향제어&lt;/li&gt;
  &lt;li&gt;GPS, Lidar 를 활용하여 주행경로, 충돌회피 등에 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;추가옵션:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;방향제어를 위한 키 추가 장착가능&lt;/li&gt;
  &lt;li&gt;사양에 맞게 배터리 옵션, 제어기 설정등 맞춤형 제작 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/PfX4jajMRxE&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.&lt;/p&gt;

&lt;h3 id=&quot;자율운항-보트-대회등의-목적에-적합하도록-기본-제어-매뉴얼-및-예제-코드-제공&quot;&gt;자율운항 보트 대회등의 목적에 적합하도록 기본 제어 매뉴얼 및 예제 코드 제공.&lt;/h3&gt;

&lt;h4 id=&quot;esc-calibration-instruction&quot;&gt;ESC Calibration instruction&lt;/h4&gt;
&lt;p&gt;초기에 제공되는 ESC 는 기본적으로 초기 Calibration 이 필요하며 Arduino 를 활용하여 모터의 Min/Max speed 설정을 하여 사용을 해야 합니다..&lt;/p&gt;

&lt;p&gt;There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://wordpress.com/&quot;&gt;wordpress&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.wix.com/&quot;&gt;wix&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.squarespace.com/&quot;&gt;squarespace&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ghost.org/&quot;&gt;ghost&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I started with the web interfaced generators with included hosting in their offerings:&lt;/p&gt;

&lt;p&gt;I have tried &lt;a href=&quot;https://www.wix.com/&quot;&gt;wix&lt;/a&gt; and &lt;a href=&quot;https://www.squarespace.com/&quot;&gt;squarespace&lt;/a&gt; before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.&lt;/p&gt;

&lt;p&gt;Finally it came self sailing boat&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;keep it all online, special &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gh-pages&lt;/code&gt; .&lt;/li&gt;
  &lt;li&gt;Have a synchronized local copy of the source files for the website.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I picked up a template, like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;SEO meta tags&lt;/li&gt;
  &lt;li&gt;Dark mode&lt;/li&gt;
  &lt;li&gt;comments ‘courtain’ to mask the disqus interface&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/template_screenshots/homepage-responsive.jpg&quot; alt=&quot;my new blog&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/template_screenshots/light-toggle.png&quot; alt=&quot;night theme toggle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites.&lt;/p&gt;

&lt;p&gt;You can use the modified template yourself by &lt;a href=&quot;https://github.com/the-mvm/the-mvm.github.io/fork/&quot;&gt;forking my repository&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;hosting&quot;&gt;Hosting&lt;/h4&gt;
&lt;p&gt;Since I decided on Jekyll to generate my site, the choice for hosting was quite obvious, &lt;strong&gt;&lt;a href=&quot;https://pages.github.com&quot;&gt;Github Pages&lt;/a&gt;&lt;/strong&gt; is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (&lt;a href=&quot;https://the-mvm.github.io&quot;&gt;the-mvm.github.io&lt;/a&gt;).&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="general blogging" /><category term="thoughts" /><category term="life" /><summary type="html">자율운항 선박개발 및 알고리즘 테스트 목적으로 RLmodel ASV-A1 을 출시, 자율운항 선박(보트) 등의 주행알고리즘 개발이나 선박주행의 센서퓨전, 기타 제어로직 연구에 적합하도록 개발되었습니다. 사양: 수중모터(Thruster) 2개의 속도제어를 이용하여 방향제어 GPS, Lidar 를 활용하여 주행경로, 충돌회피 등에 적용</summary></entry><entry><title type="html">RL Connect 프로그램 제공 - Free Ntrip client Network RTK</title><link href="https://github.com/yunbum/yunbum.github.io/deep-q-learning-tic-tac-toe.html" rel="alternate" type="text/html" title="RL Connect 프로그램 제공 - Free Ntrip client Network RTK" /><published>2021-03-19T06:14:20+09:00</published><updated>2021-03-19T06:14:20+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/deep-q-learning-tic-tac-toe</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/deep-q-learning-tic-tac-toe.html">&lt;h2 id=&quot;rtk-real-time-kinematic&quot;&gt;RTK Real Time Kinematic&lt;/h2&gt;
&lt;p&gt;GPS 의 정밀도를 높이기 위한 RTK mode 중 Network RTK 기능 설정을 위해 필요한 Ntrip client 툴을 직접 개발하여 사용하고, 별로도 독립 툴로 개발하여 배포합니다..&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210318/RL_Connect_ui.jpg&quot; alt=&quot;RL_Connect&quot; /&gt;
&lt;small&gt;[RL_Connect] Netwrok RTK 모드 설정을 위한 Ntrip client..&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;기본적으로 RLmodel 의 자율주행 차량과 자율운항 보트에는 기본 내장된 기능
일반 Ntrip client 에는 없는 모드별 분포, 비율을 계산하여 상태를 분석할 수 있도록 지원&lt;/p&gt;

&lt;center&gt;&lt;img style=&quot;float: left;margin-right: 1em;&quot; src=&quot;./assets/img/posts/20210318/tm_circle.png&quot; width=&quot;310&quot; height=&quot;320&quot; /&gt;&lt;/center&gt;

&lt;p&gt;TM 좌표계로 변환 -&amp;gt; 점들을 포함하는 최소원을 계산하여 원의 반경 계산하여 표시&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;마운트 위치 테이블 정보 수신.&lt;/li&gt;&lt;li&gt;FKP, VRS mount 기준국 접속 가능.&lt;/li&gt;&lt;li&gt;접속계정 데이타 DB로 관리&lt;/li&gt;&lt;li&gt;GPS NMEA 데이타 로거로도 적용 (위성지도 연동) &lt;/li&gt;&lt;/ul&gt;

&lt;h2 id=&quot;source-code--github&quot;&gt;Source code / Github&lt;/h2&gt;
&lt;h3 id=&quot;python---labview&quot;&gt;Python -&amp;gt; LabVIEW&lt;/h3&gt;

&lt;p&gt;참고 python 코드 깃헙 &lt;a href=&quot;https://github.com/tridge/pyUblox/blob/master/ntrip.py&quot;&gt;tridge/pyUblox&lt;/a&gt; Ntrip client 소스코드.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;GET /{} HTTP/1.1&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mountpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;Host &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;Ntrip-Version: Ntrip/2.0&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;User-Agent: NTRIP pyUblox/0.0&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;Connection: close&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;\
&lt;span class=&quot;s&quot;&gt;&quot;Authorization: Basic {}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encodingTable&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;'A'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'D'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'E'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'F'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'G'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'H'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'I'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'J'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'K'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'L'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'M'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'O'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'P'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;'Q'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'R'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'S'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'T'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'U'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'V'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'X'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'a'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'c'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;'g'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'i'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'j'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'k'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'m'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'n'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'o'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'p'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'q'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'r'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'s'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'t'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'u'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'v'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'4'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'5'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'6'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'7'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'8'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'9'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'+'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;GNSS/GP 모듈은 uBlox F9P M8P, Sententrio Mosaic X5, MBC MRP, 등에 적용하여 테스트.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210318/hw_block.JPG&quot; width=&quot;540&quot; /&gt;
&lt;small&gt;RL Connect Hardware Function block&lt;/small&gt;&lt;/center&gt;

&lt;p&gt;After &lt;strong&gt;24 hours!&lt;/strong&gt;, my computer 
&lt;a name=&quot;Model3&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;model-3---new-network-topology&quot;&gt;Model 3 - new network topology&lt;/h3&gt;

&lt;p&gt;고정 지점에서 정밀도 분석을 할 경우를 참고하여 ref x1,x2,x3 원을 그래프에 표시하게 하여 현재 데이타의 상태나 품질을 확인 할 수 있음.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fully support Any GNSS/GPS module and FKP,VRS mode&lt;/strong&gt; 다른 무료 Ntrip client 프로그램들이 제조사 판매제품만을 지원하거나 FKP/VRS 모드에 제한이 있는 것과는 차별화 해서 모두 가능하도록 지원.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ir2g4bBHfGc&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;추가 포함기능 :&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GPS 모듈 2개를 동시에 연동하여 NMEA 데이타 표시 가능&lt;/li&gt;
  &lt;li&gt;위와 같은 모드로 2 지점이 있을 경우, 두 지점간의 거리와 각도를 표시&lt;/li&gt;
  &lt;li&gt;Raw RTCM 메세지를 직접 확인가능.&lt;/li&gt;
  &lt;li&gt;로그파일을 기본으로 남기도록 하여 후에 데이타 확인 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a name=&quot;Communication&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;communication---tcpip-serial&quot;&gt;Communication - TCP/IP, Serial&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Google map api 를 활용하여 실시간 위성지도 연동&lt;/li&gt;
  &lt;li&gt;Hardware를 포함한 Network 상태 모니터링 / USB Serial, TCP, RTCM 메세지&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;기본 Base 프로그램은 LabVIEW 언어를 활용&lt;/p&gt;

&lt;p&gt;With LabVIEW implemented, &lt;strong&gt;the issue&lt;/strong&gt;
&lt;a name=&quot;LabVIEW&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;labview---national-instrument-programming-language&quot;&gt;LabVIEW - National Instrument programming language&lt;/h3&gt;

&lt;p&gt;Mode 를 지속적으로 카운팅 하여 정밀도 상태 표시:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;지정된 시간마다 모드의 상태를 카운팅 (N/A, Standalone, RTK float, RTK fixed)&lt;/li&gt;
  &lt;li&gt;카운팅 된 회수를 바탕으로 백분율로 계산하여 % 스케일로도 표시&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210318/statistics.JPG&quot; alt=&quot;tcp_block&quot; /&gt;
&lt;small&gt;[tcp_block] LabVIEW TCP Function Block Diagram code.&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;현재 지원하는 윈도우 버전 뿐만 아니라, &lt;a href=&quot;https://github.com/yunbum/NtripClient&quot;&gt;I converting RC Connect to Linux / Ubuntu version&lt;/a&gt;, Linux Ubuntu 버전 변화작업도 진행예정.&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="GPS" /><category term="Network rtk" /><category term="Ntrip client" /><category term="GNSS" /><category term="fkp" /><category term="vrs" /><summary type="html">RTK Real Time Kinematic GPS 의 정밀도를 높이기 위한 RTK mode 중 Network RTK 기능 설정을 위해 필요한 Ntrip client 툴을 직접 개발하여 사용하고, 별로도 독립 툴로 개발하여 배포합니다..</summary></entry><entry><title type="html">경로파일 GPX포맷 waypoint / GPX route editor</title><link href="https://github.com/yunbum/yunbum.github.io/neural-network-optimization-methods.html" rel="alternate" type="text/html" title="경로파일 GPX포맷 waypoint / GPX route editor" /><published>2021-03-13T04:32:20+09:00</published><updated>2021-03-13T04:32:20+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/neural-network-optimization-methods</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/neural-network-optimization-methods.html">&lt;h2 id=&quot;gpx-파일처리--gpx-format-file-handling&quot;&gt;GPX 파일처리 / GPX format file handling&lt;/h2&gt;
&lt;p&gt;SRC 기본 프로그램은 실외 자율주행 경로생성을 위한 툴로 GPX editor를 사용하고 있습니다. 위성지도 혹은 일반 지도상에 이동하려는 경로를 클릭하면 해당 지점들의 정보가 xml 포맷으로 저장되어 생성됩니다.&lt;/p&gt;

&lt;h3 id=&quot;gpx-route-editor&quot;&gt;GPX Route editor&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://www.gpsnote.net/&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210312/routeeditor-small.jpg&quot; width=&quot;540&quot; /&gt;
&lt;small&gt;GPX route editor 실행화면&lt;/small&gt;&lt;/center&gt;

&lt;h3 id=&quot;위도경도-xml-데이타-추출&quot;&gt;위도/경도 xml 데이타 추출&lt;/h3&gt;
&lt;p&gt;gpx 파일에 있는 위도/경도 데이타를 읽고 TM 좌표변환 후에 실제 주행경로 로직에 반영하여 계산합니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210312/latlong_point.png&quot; width=&quot;540&quot; /&gt;
&lt;small&gt;GPX 파일의 위도/경도 값 읽어 TM 좌표로 표시&lt;/small&gt;&lt;/center&gt;

&lt;p&gt;gpx 파일에서 TM 좌표 변환을 한 후에는 촘촘히 interpolation 하여 최종 실제 경로로 사용합니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210312/tm_interpolation.png&quot; width=&quot;540&quot; /&gt;
&lt;small&gt;TM 좌표들의 점들을 interpolation 하여 최종경로 그래프&lt;/small&gt;&lt;/center&gt;</content><author><name>Ybbaek</name></author><category term="waypoint" /><category term="gpx" /><category term="tm" /><category term="point" /><summary type="html">GPX 파일처리 / GPX format file handling SRC 기본 프로그램은 실외 자율주행 경로생성을 위한 툴로 GPX editor를 사용하고 있습니다. 위성지도 혹은 일반 지도상에 이동하려는 경로를 클릭하면 해당 지점들의 정보가 xml 포맷으로 저장되어 생성됩니다.</summary></entry><entry><title type="html">Other accessory</title><link href="https://github.com/yunbum/yunbum.github.io/ML-Library-from-scratch.html" rel="alternate" type="text/html" title="Other accessory" /><published>2021-03-01T03:32:20+09:00</published><updated>2021-03-01T03:32:20+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/ML-Library-from-scratch</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/ML-Library-from-scratch.html">&lt;p&gt;모바일 로봇에 장착하거나 연결해서 사용할 수 있는 다양한 악세사리를 제공하고 있습니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/ML_cloud.jpg&quot; width=&quot;480px&quot; /&gt;&lt;/center&gt;
&lt;p&gt;Let me try to explain; I am in the process of immersing myself into the world of Machine Learnin.&lt;/p&gt;

&lt;p&gt;Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/nnet_flow.gif&quot; /&gt;&lt;/center&gt;

&lt;p&gt;The library started very narrowly, with just the following functionality:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;create&lt;/strong&gt; a neural network based on the following parameters:
    &lt;ul&gt;
      &lt;li&gt;number of inputs&lt;/li&gt;
      &lt;li&gt;size and number of hidden layers&lt;/li&gt;
      &lt;li&gt;number of outputs&lt;/li&gt;
      &lt;li&gt;learning rate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;forward propagate&lt;/strong&gt; or predict the output values when given some inputs&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;learn&lt;/strong&gt; through back propagation using gradient descent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I restricted the model to be sequential, the only activation function I implemented was sigmoid:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/nn_diagram.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;With my neural network coded, I tested it with a very basic problem, the famous XOR problem.&lt;/p&gt;

&lt;p&gt;XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/xor_problem.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true.&lt;/p&gt;

&lt;p&gt;For the test I created a neural network with my library:&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Neural_Network&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hidden_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.03&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;NN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NeuralNetwork&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rounds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;canvas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_window_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Learning XOR Algorithm'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_size_inches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;axs1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;projection&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'3d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;axs2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:&lt;/p&gt;

&lt;p&gt;As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;axs1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_surface&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;rstride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;cstride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'viridis'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;vmin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;vmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;n&quot;&gt;antialiased&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The end result looks something like this:&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/Surface_XOR.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Then we reshape the z array as a one dimensional array to use it to color the scatter plot:&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210228/Final_XOR_Plot.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;So my baby ML library is completed for now, but still I would like to enhance it in several ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;include multiple activation functions (ReLu, linear, Tanh, etc.)&lt;/li&gt;
  &lt;li&gt;allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.)&lt;/li&gt;
  &lt;li&gt;have batch and epoch training schedules functionality&lt;/li&gt;
  &lt;li&gt;save and load trained model to file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will get to it soon…&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="accessory" /><category term="trailer" /><category term="speaker" /><category term="led" /><summary type="html">모바일 로봇에 장착하거나 연결해서 사용할 수 있는 다양한 악세사리를 제공하고 있습니다. Let me try to explain; I am in the process of immersing myself into the world of Machine Learnin.</summary></entry><entry><title type="html">Conway’s Game of Life</title><link href="https://github.com/yunbum/yunbum.github.io/conways-game-of-life.html" rel="alternate" type="text/html" title="Conway’s Game of Life" /><published>2021-02-11T04:32:20+09:00</published><updated>2021-02-11T04:32:20+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/conways-game-of-life</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/conways-game-of-life.html">&lt;p&gt;I&amp;nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&amp;nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210210/300px-TRS-80_Color_Computer_3.jpg&quot; alt=&quot;Tandy Color Computer TRS80 III&quot; /&gt;&lt;small&gt;Tandy Color Computer TRS80 III&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;For one of my starter quick programming tasks, I&amp;nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.&lt;/p&gt;

&lt;ul&gt;&lt;li&gt;If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of loneliness&lt;/li&gt;&lt;li&gt;If a cell has more than 3 neighbors, it will die of overpopulation&lt;/li&gt;&lt;li&gt;If an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spot&lt;/li&gt;&lt;li&gt;If an alive cell has 2 or 3 alive neighbors, it continues to live&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;./assets/img/posts/20210210/GameOfLife.gif&quot; alt=&quot;Conway's rules for the Game of Life&quot; /&gt;&lt;small&gt;Conway’s rules for the Game of Life&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;In the end the algorithm ended up as follows:&lt;/p&gt;

&lt;ol&gt;&lt;li&gt;Iterate through all the alive cells and get all of their neighbors&lt;/li&gt;&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neighbors&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;2&quot;&gt;&lt;li&gt;Mark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.&lt;/li&gt;&lt;/ol&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alive_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alive_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alive_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alive_cells&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alive_neighbors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alive_cells&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cell&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I&amp;nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again.&lt;/p&gt;</content><author><name>Armando Maynez</name></author><category term="coding" /><category term="python" /><summary type="html">I&amp;nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&amp;nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.</summary></entry><entry><title type="html">Single Neuron Perceptron</title><link href="https://github.com/yunbum/yunbum.github.io/single-neuron-perceptron.html" rel="alternate" type="text/html" title="Single Neuron Perceptron" /><published>2021-01-26T04:32:20+09:00</published><updated>2021-01-26T04:32:20+09:00</updated><id>https://github.com/yunbum/yunbum.github.io/single-neuron-perceptron</id><content type="html" xml:base="https://github.com/yunbum/yunbum.github.io/single-neuron-perceptron.html">&lt;p&gt;As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-perceptron&quot;&gt;What is a perceptron?&lt;/h2&gt;

&lt;p&gt;A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.&lt;/p&gt;

&lt;p&gt;Back in the late 1950’s, a young &lt;a href=&quot;https://en.wikipedia.org/wiki/Frank_Rosenblatt&quot;&gt;Frank Rosenblatt&lt;/a&gt; devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.&lt;/p&gt;

&lt;p&gt;In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210125/Perceptron.png&quot; /&gt;&lt;/center&gt;

&lt;p&gt;Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.&lt;/p&gt;

&lt;p&gt;In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.&lt;/p&gt;

&lt;p&gt;For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \; x_2 = 2\; and \; x_3 = 3$, if it’s weights are $w_1 = 0.5,\; w_2 = 1\; and \; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;\(&lt;br /&gt;
\begin{align}
\begin{split}
\left(x_1 * w_1\right) + \left(x_2 * w_2\right) + \left(x_3 * w_3\right)
\end{split}
\end{align}
\)&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;\(&lt;br /&gt;
\begin{align}&lt;br /&gt;
\begin{split}&lt;br /&gt;
\left(0.5 * 1\right) + \left(1 * 2\right) + \left(-1 * 3\right) = 0.5 + 2 - 3 = -0.5
\end{split}&lt;br /&gt;
\end{align}&lt;br /&gt;
\)&lt;/p&gt;

&lt;p&gt;Typically when this value is obtained, we need to apply an “activation” function to smooth the output.&lt;/p&gt;

&lt;p&gt;For this we need a set of data that it is already classified, we call this a training set.&lt;/p&gt;

&lt;p&gt;The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. &lt;a href=&quot;https://www.youtube.com/watch?v=aircAruvnKk&amp;amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&quot;&gt;This video series by 3 blue 1 brown explains it wonderfuly.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The neuron has 3 inputs and weights to calculate its output:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;input 1 is the X coordinate of the point,
Input 2 is the y coordinate of the point,
Input 3 is the bias and it is always 1

Input 3 or the bias is required for lines that do not cross the origin (0,0)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.&lt;/p&gt;

&lt;p&gt;The output of the perceptron is calculated with the following activation function:
    if x * weight_x + y weight_y + weight_bias is positive then 1 else 0&lt;/p&gt;

&lt;p&gt;The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Expected&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Calculated&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Error&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With every point that is learned if the error is not 0 the weights are adjusted according to:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;New_weight = Old_weight + error * input * learning_rate
for example: New_weight_x = Old_weight_x + error * x * learning rate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this particular case, I coded the learning_rate to decrease with every iteration as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;learning_rate = 0.01 / (iteration + 1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;./assets/img/posts/20210125/Learning_1000_points_per_iteration.jpg&quot; /&gt;&lt;/center&gt;

&lt;p&gt;In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.&lt;/p&gt;</content><author><name>Ybbaek</name></author><category term="machine learning" /><category term="coding" /><category term="neural networks" /><summary type="html">As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.</summary></entry></feed>