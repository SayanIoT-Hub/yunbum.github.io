[
  
    {
      "title"       : "RLmodel 자율주행 차량 이력 / History!",
      "category"    : "opinion",
      "tags"        : "copyright, creativity, neural networks, machine learning, artificial intelligence",
      "url"         : "./AI-and-intellectual-property.html",
      "date"        : "2021-04-20 00:00:00 +0900",
      "description" : "자율주행관련 Mobile robot 개발자로써 관련 알고리즘 개발, 센서퓨전 테스트 등의 개발을 하는데 있어 크고 무거운 챠량 혹은 로봇등을 사용하는데 어려움을 많이 격게되어 휴대성이 좋고 쉽게 테스트,개발을 할수 있는 플랫폼을 만들게되었습니다.",
      "content"     : "자율주행 모바일 로봇개발 목적으로 개발한 플랫폼 check github repo .Legacy SRC Mobile robots 테스트 차량들[SRC history] 여러종류의 RC 차량의 개조와 테스트를 거처, 모든 구성 부품들이 완전히 개조되고 보완되어 업그레이드 되어 개발되었습니다..SRC 로봇은 약 소형 RC 자동차 개량으로 출발해서, 다양한 RC 차량을 별도의 모터드라이버, 아두이노, 라즈베리파이, 카메라, 라이다 등을 부착하여 튜닝하는 것을 시작으로, GPS, Lidar 등을 추가하고 Waypoint 제어등을 거쳐 현재의 완벽한 자율주행 차량으로 업그레이드 되게 되었습니다.[Metal frame] 전체 메탈 프레임 초기 모델중 하나.다양한 프레임 소재 및 차량 종류의 테스트를 거처 최종으로 전체 메탈프레임 구조를 갖추고 완성이 되게 되어 매우 강한 강성과 함께 파손우려가 없고 수리, 보완에도 용이하도록 개발이 완료되었습니다.So as we currently stand, a human author is required to grant a copyright, which makes sense, there is no point of having a neural network be the beneficiary of royalties of a creative work (no bank would open an account for them anyways, lol).The Next SRC 는 보다 강력한 모터, 주행알고리즘 등으로 개량되어, 커리큘럼 및 예제와 함께 (Python, LabVIEW 등) 모바일 버전으로도 준비중에 있습니다. ."
    } ,
  
    {
      "title"       : "자율주행 플랫폼 SRC 란, what is SRC (Self Driving Rc Car?",
      "category"    : "theory",
      "tags"        : "Robot, RC, GPS, Camera, Lidar",
      "url"         : "./back-to-basics.html",
      "date"        : "2021-04-02 00:00:00 +0900",
      "description" : "SRC 모바일로봇 이란: SRC Intro.",
      "content"     : "SRC 은 GPS, Camera, Lidar, IMU, 등의 센서를 기반으로 자율주행 교육 및 주행알고리즘, 센서퓨전 등을 쉽고 효과적으로 개발 할 수 있는 자율주행 모바일로봇 플랫폼 입니다.: SRC 는 자율주행 알고리즘 및 교육목적으로 개발 되었습니다. 10여년 간 네이버 카페를 운영하여 관련 예제나 테스트 결과등을 공유하고 있습니다. 다른 RLmodel 유투브 영상들다양한 HW 및 SW 모듈로 주행알고리즘 검증 및 센서퓨전 테스트에 적용완료 차량제어는 Python, C, 등의 컴퓨터언어와도 호환이 되도록 시리얼통신으로 제어할 수 있습니다. LabVIEW 라고하는 National Instrument 사의 프로그래밍 언어도 지원합니다.소개/Intro모델구분 및 특징SRC (Self driving Remote control Car) 는 자율주행 차량의 영어 약자 이면 현재 SRC-A,B,C,D 등의 타입이 있습니다.타입별 차이는 크기, 속도, 모터토크 등으로 구분되어 나누어 집니다.개발시간 전체 약 10여년. 최종 전체 금속기반의 구조는 2019년 완성주요특징프레임은 전체가 알루미늄, 철 등의 금속. 일반 휴대용 보조배터리 파워로 일반 USB 충전이 가능하여 편리하며. 통 고무 타이어로 구성되어 펑크날 위험이 없습니다. (source), 구조와 강성이 상당히 개량되어 전체적인 안정성은 일반 플라스틱 RC 카에 비할 수 없고, 현재 수십 km 실외 운행으로 내구성 검증도 완료:개발시간 전체 약 10여년. 최종 전체 금속기반의 구조는 2019년 완성모듈/센서개별 모듈들의 구성은 맞춤형으로 제공 될 수 있으며, 기본 모터와 모터드라이버, 상위제어기(아두이노) 만으로도 제공이 되며 제어명령은 USB 을 통한 시리얼 통신으로 제어가 가능합니다..Full option 상태의 센서 및 기타 모듈 구성도튜닝/ 업그레이드동일 차량모델 (A,B,C,D) 에 속도, 토크를 2배로 향상할 수 있는 듀얼모터 (Dual moter) 옵션으로 선택이 가능하여 등판능력, 속도 등을 높힘 모델로 선택이 가능합니다.프레임 강성을 유지하기 위해 보완 패치들로 접속부분 추가 체결함모바일 로봇의 종류는 기본 SRC 차량모델에 이어 ASV 보트(선박) 모델도 있으며 교육목적의 Pendulum, Ball Balance robot 등도 있습니다.강화학습 적용 플랫폼모든 RLmodel 로봇들은 강화학습을 적용하고 테스트 해볼 수 있는 목적으로 개발이 시작되었으며 현재도 다양한 접근으로 예제들을 검토 중입니다.기타 편의사항SRC 차량은 간단한 시리얼 프로토콜 제어가 가능하고 부가 기능으로는 스피커, LED dot matrix, Light 등을 옵션으로 창작하여 테스트 시 차량의 상태나 프로그램의 동작 세부사항 확인이 쉽도록 하였습니다.야간주행 테스트를 위한 LED light추가적으로 ROS 환경에 적합하도록 패키지도 개발 중 입니다."
    } ,
  
    {
      "title"       : "ASV 자율운항 보트 / Autonomous Surface Vehicle",
      "category"    : "",
      "tags"        : "general blogging, thoughts, life",
      "url"         : "./starting-the-adventure.html",
      "date"        : "2021-03-24 00:00:00 +0900",
      "description" : "무인 자율운항 보트/선박 ASV",
      "content"     : "자율운항 선박개발 및 알고리즘 테스트 목적으로 RLmodel ASV-A1 을 출시, 자율운항 선박(보트) 등의 주행알고리즘 개발이나 선박주행의 센서퓨전, 기타 제어로직 연구에 적합하도록 개발되었습니다.사양: 수중모터(Thruster) 2개의 속도제어를 이용하여 방향제어 GPS, Lidar 를 활용하여 주행경로, 충돌회피 등에 적용추가옵션: 방향제어를 위한 키 추가 장착가능 사양에 맞게 배터리 옵션, 제어기 설정등 맞춤형 제작 가능For my next project I think I will start to do the basic hand-written digits recognition, which is the Machine Learning Hello World, for this I think I will start to use Tensorflow already.자율운항 보트 대회등의 목적에 적합하도록 기본 제어 매뉴얼 및 예제 코드 제공.ESC Calibration instruction초기에 제공되는 ESC 는 기본적으로 초기 Calibration 이 필요하며 Arduino 를 활용하여 모터의 Min/Max speed 설정을 하여 사용을 해야 합니다..There is a myriad of website generators nowadays, after a lengthy search the ones I ended up considering are: wordpress wix squarespace ghostI started with the web interfaced generators with included hosting in their offerings:I have tried wix and squarespace before, they are fantastic for quick and easy website generation, but their free offering has ads, so again, a big no for me.Finally it came self sailing boat keep it all online, special gh-pages . Have a synchronized local copy of the source files for the website.I picked up a template, like: SEO meta tags Dark mode comments ‘courtain’ to mask the disqus interfaceAs a summary, Hugo and Gatsby might be much faster than Jekyll to build the sites.You can use the modified template yourself by forking my repository.HostingSince I decided on Jekyll to generate my site, the choice for hosting was quite obvious, Github Pages is very nicely integrated with it, it is free, and it has no ads! Plus the domain name isn’t too terrible (the-mvm.github.io)."
    } ,
  
    {
      "title"       : "RL Connect 프로그램 제공 - Free Ntrip client Network RTK",
      "category"    : "",
      "tags"        : "GPS, Network rtk, Ntrip client, GNSS, fkp, vrs",
      "url"         : "./deep-q-learning-tic-tac-toe.html",
      "date"        : "2021-03-19 06:14:20 +0900",
      "description" : "Free Ntrip client 'RL Connect'",
      "content"     : "RTK Real Time KinematicGPS 의 정밀도를 높이기 위한 RTK mode 중 Network RTK 기능 설정을 위해 필요한 Ntrip client 툴을 직접 개발하여 사용하고, 별로도 독립 툴로 개발하여 배포합니다..[RL_Connect] Netwrok RTK 모드 설정을 위한 Ntrip client..기본적으로 RLmodel 의 자율주행 차량과 자율운항 보트에는 기본 내장된 기능일반 Ntrip client 에는 없는 모드별 분포, 비율을 계산하여 상태를 분석할 수 있도록 지원TM 좌표계로 변환 -&gt; 점들을 포함하는 최소원을 계산하여 원의 반경 계산하여 표시마운트 위치 테이블 정보 수신.FKP, VRS mount 기준국 접속 가능.접속계정 데이타 DB로 관리GPS NMEA 데이타 로거로도 적용 (위성지도 연동) Source code / GithubPython -&gt; LabVIEW참고 python 코드 깃헙 tridge/pyUblox Ntrip client 소스코드.header =\\\"GET /{} HTTP/1.1\\r\\n\".format(mountpoint) +\\\"Host \\r\\n\".format(server) +\\\"Ntrip-Version: Ntrip/2.0\\r\\n\" +\\\"User-Agent: NTRIP pyUblox/0.0\\r\\n\" +\\\"Connection: close\\r\\n\" +\\\"Authorization: Basic {}\\r\\n\\r\\n\".format(pwd)static const char encodingTable [64] = { 'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P', 'Q','R','S','T','U','V','W','X','Y','Z','a','b','c','d','e','f', 'g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v', 'w','x','y','z','0','1','2','3','4','5','6','7','8','9','+','/'}GNSS/GP 모듈은 uBlox F9P M8P, Sententrio Mosaic X5, MBC MRP, 등에 적용하여 테스트.RL Connect Hardware Function blockAfter 24 hours!, my computer Model 3 - new network topology고정 지점에서 정밀도 분석을 할 경우를 참고하여 ref x1,x2,x3 원을 그래프에 표시하게 하여 현재 데이타의 상태나 품질을 확인 할 수 있음.Fully support Any GNSS/GPS module and FKP,VRS mode 다른 무료 Ntrip client 프로그램들이 제조사 판매제품만을 지원하거나 FKP/VRS 모드에 제한이 있는 것과는 차별화 해서 모두 가능하도록 지원.추가 포함기능 : GPS 모듈 2개를 동시에 연동하여 NMEA 데이타 표시 가능 위와 같은 모드로 2 지점이 있을 경우, 두 지점간의 거리와 각도를 표시 Raw RTCM 메세지를 직접 확인가능. 로그파일을 기본으로 남기도록 하여 후에 데이타 확인 가능Communication - TCP/IP, Serial Google map api 를 활용하여 실시간 위성지도 연동 Hardware를 포함한 Network 상태 모니터링 / USB Serial, TCP, RTCM 메세지기본 Base 프로그램은 LabVIEW 언어를 활용With LabVIEW implemented, the issueLabVIEW - National Instrument programming languageMode 를 지속적으로 카운팅 하여 정밀도 상태 표시: 지정된 시간마다 모드의 상태를 카운팅 (N/A, Standalone, RTK float, RTK fixed) 카운팅 된 회수를 바탕으로 백분율로 계산하여 % 스케일로도 표시[tcp_block] LabVIEW TCP Function Block Diagram code.현재 지원하는 윈도우 버전 뿐만 아니라, I converting RC Connect to Linux / Ubuntu version, Linux Ubuntu 버전 변화작업도 진행예정."
    } ,
  
    {
      "title"       : "로그파일 분석 / Log file analysis after driving",
      "category"    : "",
      "tags"        : "log, NMEA, replay, statistics",
      "url"         : "./neural-network-optimization-methods.html",
      "date"        : "2021-03-13 04:32:20 +0900",
      "description" : "Log file analysis tool.",
      "content"     : "For the seemingly small project I undertook of creating a machine learning neural network that could learn by itself to play tic-tac-toe, I bumped into the necesity of implementing at least one momentum algorithm for the optimization of the network during backpropagation.And since my original post for the TicTacToe project is quite large already, I decided to post separately these optimization methods and how did I implement them in my code.AdamsourceAdaptive Moment Estimation (Adam) is an optimization method that computes adaptive learning rates for each weight and bias. In addition to storing an exponentially decaying average of past squared gradients \\(v_t\\) and an exponentially decaying average of past gradients \\(m_t\\), similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients \\(m_t\\) and \\(v_t\\) respectively as follows:\\(\\begin{align}\\begin{split}m_t &amp;= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\v_t &amp;= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\end{split}\\end{align}\\)\\(m_t\\) and \\(v_t\\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As \\(m_t\\) and \\(v_t\\) are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. \\(\\beta_1\\) and \\(\\beta_2\\) are close to 1).They counteract these biases by computing bias-corrected first and second moment estimates:\\(\\begin{align}\\begin{split}\\hat{m}_t &amp;= \\dfrac{m_t}{1 - \\beta^t_1} \\\\\\hat{v}_t &amp;= \\dfrac{v_t}{1 - \\beta^t_2} \\end{split}\\end{align}\\)We then use these to update the weights and biases which yields the Adam update rule:\\(\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\).The authors propose defaults of 0.9 for \\(\\beta_1\\), 0.999 for \\(\\beta_2\\), and \\(10^{-8}\\) for \\(\\epsilon\\).view on github# decaying averages of past gradientsself.v[\"dW\" + str(i)] = ((c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) ))self.v[\"db\" + str(i)] = ((c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) ))# decaying averages of past squared gradientsself.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square(np.array( self.bias_gradients[i]))) ))if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch))# apply to weights and biasesweight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON))))self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON))))SGD MomentumsourceVanilla SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction \\(\\gamma\\) of the update vector of the past time step to the current update vector:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)The momentum term \\(\\beta_1\\) is usually set to 0.9 or a similar value.Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. \\(\\beta_1 &lt; 1\\)). The same thing happens to our weight and biases updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.view on githubself.v[\"dW\"+str(i)] = ((c.BETA1*self.v[\"dW\" + str(i)]) +(eta*np.array(self.gradients[i]) ))self.v[\"db\"+str(i)] = ((c.BETA1*self.v[\"db\" + str(i)]) +(eta*np.array(self.bias_gradients[i]) ))weight_col -= self.v[\"dW\" + str(i)]self.bias[i] -= self.v[\"db\" + str(i)]Nesterov accelerated gradient (NAG)sourceHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We'd like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience. We know that we will use our momentum term \\(\\beta_1 v_{t-1}\\) to move the weights and biases \\(\\theta\\). Computing \\( \\theta - \\beta_1 v_{t-1} \\) thus gives us an approximation of the next position of the weights and biases (the gradient is missing for the full update), a rough idea where our weights and biases are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current weights and biases \\(\\theta\\) but w.r.t. the approximate future position of our weights and biases:\\(\\begin{align}\\begin{split}v_t &amp;= \\beta_1 v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\beta_1 v_{t-1} ) \\\\\\theta &amp;= \\theta - v_t\\end{split}\\end{align}\\)Again, we set the momentum term \\(\\beta_1\\) to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update. This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of Neural Networks on a number of tasks.Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual weight and bias to perform larger or smaller updates depending on their importance.view on githubv_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]}self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i]))self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i]))weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)])self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)])RMSpropsourceRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.RMSprop was developed stemming from the need to resolve other method's radically diminishing learning rates.\\(\\begin{align}\\begin{split}E[\\theta^2]_t &amp;= \\beta_1 E[\\theta^2]_{t-1} + (1-\\beta_1) \\theta^2_t \\\\\\theta_{t+1} &amp;= \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[\\theta^2]_t + \\epsilon}} \\theta_{t}\\end{split}\\end{align}\\)RMSprop divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests \\(\\beta_1\\) to be set to 0.9, while a good default value for the learning rate \\(\\eta\\) is 0.001.view on githubself.s[\"dW\" + str(i)] = ((c.BETA1 * self.s[\"dW\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.gradients[i]))) ))self.s[\"db\" + str(i)] = ((c.BETA1 * self.s[\"db\" + str(i)]) + ((1-c.BETA1) * (np.square(np.array(self.bias_gradients[i]))) ))weight_col -= (eta * (np.array(self.gradients[i]) / (np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) )self.bias[i] -= (eta * (np.array(self.bias_gradients[i]) / (np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) )Complete codeAll in all the code ended up like this:view on github@staticmethoddef cyclic_learning_rate(learning_rate, epoch): max_lr = learning_rate * c.MAX_LR_FACTOR cycle = np.floor(1 + (epoch / (2 * c.LR_STEP_SIZE)) ) x = np.abs((epoch / c.LR_STEP_SIZE) - (2 * cycle) + 1) return learning_rate + (max_lr - learning_rate) * np.maximum(0, (1 - x))def apply_gradients(self, epoch): true_epoch = epoch - c.BATCH_SIZE eta = self.learning_rate * (1 / (1 + c.DECAY_RATE * true_epoch)) if c.CLR_ON: eta = self.cyclic_learning_rate(eta, true_epoch) for i, weight_col in enumerate(self.weights): if c.OPTIMIZATION == 'vanilla': weight_col -= eta * np.array(self.gradients[i]) / c.BATCH_SIZE self.bias[i] -= eta * np.array(self.bias_gradients[i]) / c.BATCH_SIZE elif c.OPTIMIZATION == 'SGD_momentum': self.v[\"dW\"+str(i)] = ((c.BETA1 *self.v[\"dW\" + str(i)]) +(eta *np.array(self.gradients[i]) )) self.v[\"db\"+str(i)] = ((c.BETA1 *self.v[\"db\" + str(i)]) +(eta *np.array(self.bias_gradients[i]) )) weight_col -= self.v[\"dW\" + str(i)] self.bias[i] -= self.v[\"db\" + str(i)] elif c.OPTIMIZATION == 'NAG': v_prev = {\"dW\" + str(i): self.v[\"dW\" + str(i)], \"db\" + str(i): self.v[\"db\" + str(i)]} self.v[\"dW\" + str(i)] = (c.NAG_COEFF * self.v[\"dW\" + str(i)] - eta * np.array(self.gradients[i])) self.v[\"db\" + str(i)] = (c.NAG_COEFF * self.v[\"db\" + str(i)] - eta * np.array(self.bias_gradients[i])) weight_col += ((-1 * c.BETA1 * v_prev[\"dW\" + str(i)]) + (1 + c.BETA1) * self.v[\"dW\" + str(i)]) self.bias[i] += ((-1 * c.BETA1 * v_prev[\"db\" + str(i)]) + (1 + c.BETA1) * self.v[\"db\" + str(i)]) elif c.OPTIMIZATION == 'RMSProp': self.s[\"dW\" + str(i)] = ((c.BETA1 *self.s[\"dW\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA1 *self.s[\"db\" + str(i)]) +((1-c.BETA1) *(np.square(np.array(self.bias_gradients[i]))) )) weight_col -= (eta *(np.array(self.gradients[i]) /(np.sqrt(self.s[\"dW\"+str(i)]+c.EPSILON))) ) self.bias[i] -= (eta *(np.array(self.bias_gradients[i]) /(np.sqrt(self.s[\"db\"+str(i)]+c.EPSILON))) ) if c.OPTIMIZATION == \"ADAM\": # decaying averages of past gradients self.v[\"dW\" + str(i)] = (( c.BETA1 * self.v[\"dW\" + str(i)]) + ((1 - c.BETA1) * np.array(self.gradients[i]) )) self.v[\"db\" + str(i)] = (( c.BETA1 * self.v[\"db\" + str(i)]) + ((1 - c.BETA1) * np.array(self.bias_gradients[i]) )) # decaying averages of past squared gradients self.s[\"dW\" + str(i)] = ((c.BETA2 * self.s[\"dW\"+str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.gradients[i]))) )) self.s[\"db\" + str(i)] = ((c.BETA2 * self.s[\"db\" + str(i)]) + ((1 - c.BETA2) * (np.square( np.array( self.bias_gradients[i]))) )) if c.ADAM_BIAS_Correction: # bias-corrected first and second moment estimates self.v[\"dW\" + str(i)] = self.v[\"dW\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.v[\"db\" + str(i)] = self.v[\"db\" + str(i)] / (1 - (c.BETA1 ** true_epoch)) self.s[\"dW\" + str(i)] = self.s[\"dW\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) self.s[\"db\" + str(i)] = self.s[\"db\" + str(i)] / (1 - (c.BETA2 ** true_epoch)) # apply to weights and biases weight_col -= ((eta * (self.v[\"dW\" + str(i)] / (np.sqrt(self.s[\"dW\" + str(i)]) + c.EPSILON)))) self.bias[i] -= ((eta * (self.v[\"db\" + str(i)] / (np.sqrt(self.s[\"db\" + str(i)]) + c.EPSILON)))) self.gradient_zeros()"
    } ,
  
    {
      "title"       : "Machine Learning Library in Python from scratch",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks, python",
      "url"         : "./ML-Library-from-scratch.html",
      "date"        : "2021-03-01 03:32:20 +0900",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "It must sound crazy that in this day and age, when we have such a myriad of amazing machine learning libraries and toolkits all open sourced, all quite well documented and easy to use, I decided to create my own ML library from scratch.Let me try to explain; I am in the process of immersing myself into the world of Machine Learning, and to do so, I want to deeply understand the basic concepts and its foundations, and I think that there is no better way to do so than by creating myself all the code for a basic neural network library from scratch. This way I can gain in depth understanding of the math that underpins the ML algorithms.Another benefit of doing this is that since I am also learning Python, the experiment brings along good exercise for me.To call it a Machine Learning Library is perhaps a bit of a stretch, since I just intended to create a multi-neuron, multi-layered perceptron.The library started very narrowly, with just the following functionality: create a neural network based on the following parameters: number of inputs size and number of hidden layers number of outputs learning rate forward propagate or predict the output values when given some inputs learn through back propagation using gradient descentI restricted the model to be sequential, and the layers to be only dense / fully connected, this means that every neuron is connected to every neuron of the following layer. Also, as a restriction, the only activation function I implemented was sigmoid:With my neural network coded, I tested it with a very basic problem, the famous XOR problem.XOR is a logical operation that cannot be solved by a single perceptron because of its linearity restriction:As you can see, when plotted in an X,Y plane, the logical operators AND and OR have a line that can clearly separate the points that are false from the ones that are true, hence a perceptron can easily learn to classify them; however, for XOR there is no single straight line that can do so, therefore a multilayer perceptron is needed for the task.For the test I created a neural network with my library:import Neural_Network as nninputs = 3hidden_layers = [2, 1]outputs = 1learning_rate = 0.03NN = nn.NeuralNetwork(inputs, hidden_layers, outputs, learning_rate)The three inputs I decided to use (after a lot of trial and error) are the X and Y coordinate of a point (between X = 0, X = 1, Y = 0 and Y = 1) and as the third input the multiplication of both X and Y. Apparently it gives the network more information, and it ends up converging much more quickly with this third input.Then there is a single hidden layer with 2 neurons and one output value, that will represent False if the value is closer to 0 or True if the value is closer to 1.Then I created the learning data, which is quite trivial for this problem, since we know very easily how to compute XOR.training_data = []for n in range(learning_rounds): x = rnd.random() y = rnd.random() training_data.append([x, y, x * y, 0 if (x &lt; 0.5 and y &lt; 0.5) or (x &gt;= 0.5 and y &gt;= 0.5) else 1])And off we go into training:for data in training_data: NN.train(data[:3].reshape(inputs), data[3:].reshape(outputs))The ML library can only train on batches of 1 (another self-imposed coding restriction), therefore only one “observation” at a time, this is why the train function accepts two parameters, one is the inputs packed in an array, and the other one is the outputs, packed as well in an array.To see the neural net in action I decided to plot the predicted results in both a 3d X,Y,Z surface plot (z being the network’s predicted value), and a scatter plot with the color of the points representing the predicted value.This was plotted in MatPlotLib, so we needed to do some housekeeping first:fig = plt.figure()fig.canvas.set_window_title('Learning XOR Algorithm')fig.set_size_inches(11, 6)axs1 = fig.add_subplot(1, 2, 1, projection='3d')axs2 = fig.add_subplot(1, 2, 2)Then we need to prepare the data to be plotted by generating X and Y values distributed between 0 and 1, and having the network calculate the Z value:x = np.linspace(0, 1, num_surface_points)y = np.linspace(0, 1, num_surface_points)x, y = np.meshgrid(x, y)z = np.array(NN.forward_propagation([x, y, x * y])).reshape(num_surface_points, num_surface_points)As you can see, the z values array is reshaped as a 2d array of shape (x,y), since this is the way Matplotlib interprets it as a surface:axs1.plot_surface(x, y, z, rstride=1, cstride=1, cmap='viridis', vmin=0, vmax=1, antialiased=True)The end result looks something like this:Then we reshape the z array as a one dimensional array to use it to color the scatter plot:z = z.reshape(num_surface_points ** 2)scatter = axs2.scatter(x, y, marker='o', s=40, c=z.astype(float), cmap='viridis', vmin=0, vmax=1)To actually see the progress while learning, I created a Matplotlib animation, and it is quite interesting to see as it learns. So my baby ML library is completed for now, but still I would like to enhance it in several ways: include multiple activation functions (ReLu, linear, Tanh, etc.) allow for multiple optimizers (Adam, RMSProp, SGD Momentum, etc.) have batch and epoch training schedules functionality save and load trained model to fileI will get to it soon…"
    } ,
  
    {
      "title"       : "Conway&#39;s Game of Life",
      "category"    : "",
      "tags"        : "coding, python",
      "url"         : "./conways-game-of-life.html",
      "date"        : "2021-02-11 04:32:20 +0900",
      "description" : "Taking on the challenge of picking up coding again through interesting small projects, this time it is the turn of Conway's Game of Life.",
      "content"     : "I&nbsp;am lately trying to take on coding again. It had always been a part of my life since my early years when I&nbsp;learned to program a Tandy Color Computer at the age of 8, the good old days.Tandy Color Computer TRS80 IIIHaving already programed in Java, C# and of course BASIC, I&nbsp;thought it would be a great idea to learn Python since I&nbsp;have great interest in data science and machine learning, and those two topics seem to have an avid community within Python coders.For one of my starter quick programming tasks, I&nbsp;decided to code Conway's Game of Life, a very simple cellular automata that basically plays itself.The game consists of a grid of n size, and within each block of the grid a cell could either be dead or alive according to these rules:If a cell has less than 2 neighbors, meaning contiguous alive cells, the cell will die of lonelinessIf a cell has more than 3 neighbors, it will die of overpopulationIf an empty block has exactly 3 contiguous alive neighbors, a new cell will be born in that spotIf an alive cell has 2 or 3 alive neighbors, it continues to liveConway’s rules for the Game of LifeTo make it more of a challenge I&nbsp;also decided to implement an \"sparse\" method of recording the game board, this means that instead of the typical 2d array representing the whole board, I&nbsp;will only record the cells which are alive. Saving a lot of memory space and processing time, while adding some spice to the challenge.The trickiest part was figuring out how to calculate which empty blocks had exactly 3 alive neighbors so that a new cell will spring to life there, this is trivial in the case of recording the whole grid, because we just iterate all over the board and find the alive neighbors of ALL&nbsp;the blocks in the grid, but in the case of only keeping the alive cells proved quite a challenge.In the end the algorithm ended up as follows:Iterate through all the alive cells and get all of their neighborsdef get_neighbors(self, cell): neighbors = [] for x in range(-1, 2, 1): for y in range(-1, 2, 1): if not (x == 0 and y == 0): if (0 &amp;lt;= (cell[0] + x) &amp;lt;= self.size_x) and (0 &amp;lt;= (cell[1] + y) &amp;lt;= self.size_y): neighbors.append((cell[0] + x, cell[1] + y)) return neighborsMark all the neighboring blocks as having +1 neighbor each time a particular cell is encountered. This way, for each neighboring alive cell the counter of the particular block will increase, and in the end it will contain the total number of live cells which are contiguous to it.def next_state(self): alive_neighbors = {} for cell in self.alive_cells: if cell not in alive_neighbors: alive_neighbors[cell] = 0 neighbors = self.get_neighbors(cell) for neighbor in neighbors: if neighbor not in alive_neighbors: alive_neighbors[neighbor] = 1 else: alive_neighbors[neighbor] += 1The trick was using a dictionary to keep the record of the blocks that have alive neighbors and the cells who are alive in the current state but have zero alive neighbors (thus will die).With the dictionary it became easy just to add cells and increase their neighbor counter each time it was encountered as a neighbor of an alive cell.Having the dictionary now filled with all the cells that have alive neighbors and how many they have, it was just a matter of applying the rules of the game:for cell in alive_neighbors: if alive_neighbors[cell] &amp;lt; 2 or alive_neighbors[cell] &gt; 3: self.alive_cells.discard(cell) elif alive_neighbors[cell] == 3: self.alive_cells.add(cell)Notice that since I am keeping an array of the coordinates of only the cells who are alive, I could apply just 3 rules, die of loneliness, die of overpopulation and become alive from reproduction (exactly 3 alive neighbors) because the ones who have 2 or 3 neighbors and are already alive, can remain alive in the next iteration.I&nbsp;found it very interesting to implement the Game of Life like this, it was quite a refreshing challenge and I am beginning to feel my coding skills ramping up again."
    } ,
  
    {
      "title"       : "Single Neuron Perceptron",
      "category"    : "",
      "tags"        : "machine learning, coding, neural networks",
      "url"         : "./single-neuron-perceptron.html",
      "date"        : "2021-01-26 04:32:20 +0900",
      "description" : "Single neuron perceptron that classifies elements learning quite quickly.",
      "content"     : "As an entry point to learning python and getting into Machine Learning, I decided to code from scratch the Hello World! of the field, a single neuron perceptron.What is a perceptron?A perceptron is the basic building block of a neural network, it can be compared to a neuron, And its conception is what detonated the vast field of Artificial Intelligence nowadays.Back in the late 1950’s, a young Frank Rosenblatt devised a very simple algorithm as a foundation to construct a machine that could learn to perform different tasks.In its essence, a perceptron is nothing more than a collection of values and rules for passing information through them, but in its simplicity lies its power.Imagine you have a ‘neuron’ and to ‘activate’ it, you pass through several input signals, each signal connects to the neuron through a synapse, once the signal is aggregated in the perceptron, it is then passed on to one or as many outputs as defined. A perceptron is but a neuron and its collection of synapses to get a signal into it and to modify a signal to pass on.In more mathematical terms, a perceptron is an array of values (let’s call them weights), and the rules to apply such values to an input signal.For instance a perceptron could get 3 different inputs as in the image, lets pretend that the inputs it receives as signal are: $x_1 = 1, \\; x_2 = 2\\; and \\; x_3 = 3$, if it’s weights are $w_1 = 0.5,\\; w_2 = 1\\; and \\; w_3 = -1$ respectively, then what the perceptron will do when the signal is received is to multiply each input value by its corresponding weight, then add them up.\\(\\begin{align}\\begin{split}\\left(x_1 * w_1\\right) + \\left(x_2 * w_2\\right) + \\left(x_3 * w_3\\right)\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\left(0.5 * 1\\right) + \\left(1 * 2\\right) + \\left(-1 * 3\\right) = 0.5 + 2 - 3 = -0.5\\end{split}\\end{align}\\)Typically when this value is obtained, we need to apply an “activation” function to smooth the output, but let’s say that our activation function is linear, meaning that we keep the value as it is, then that’s it, that is the output of the perceptron, -0.5.In a practical application, the output means something, perhaps we want our perceptron to classify a set of data and if the perceptron outputs a negative number, then we know the data is of type A, and if it is a positive number then it is of type B.Once we understand this, the magic starts to happen through a process called backpropagation, where we “educate” our tiny one neuron brain to have it learn how to do its job.The magic starts to happen through a process called backpropagation, where we \"educate\" our tiny one neuron brain to have it learn how to do its job.For this we need a set of data that it is already classified, we call this a training set. This data has inputs and their corresponding correct output. So we can tell the little brain when it misses in its prediction, and by doing so, we also adjust the weights a bit in the direction where we know the perceptron committed the mistake hoping that after many iterations like this the weights will be so that most of the predictions will be correct.After the model trains successfully we can have it classify data it has never seen before, and we have a fairly high confidence that it will do so correctly.The math behind this magical property of the perceptron is called gradient descent, and is just a bit of differential calculus that helps us convert the error the brain is having into tiny nudges of value of the weights towards their optimum. This video series by 3 blue 1 brown explains it wonderfuly.My program creates a single neuron neural network tuned to guess if a point is above or below a randomly generated line and generates a visualization based on graphs to see how the neural network is learning through time.The neuron has 3 inputs and weights to calculate its output:input 1 is the X coordinate of the point,Input 2 is the y coordinate of the point,Input 3 is the bias and it is always 1Input 3 or the bias is required for lines that do not cross the origin (0,0)The Perceptron starts with weights all set to zero and learns by using 1,000 random points per each iteration.The output of the perceptron is calculated with the following activation function: if x * weight_x + y weight_y + weight_bias is positive then 1 else 0The error for each point is calculated as the expected outcome of the perceptron minus the real outcome therefore there are only 3 possible error values: Expected Calculated Error 1 -1 1 1 1 0 -1 -1 0 -1 1 -1 With every point that is learned if the error is not 0 the weights are adjusted according to:New_weight = Old_weight + error * input * learning_ratefor example: New_weight_x = Old_weight_x + error * x * learning rateA very useful parameter in all of neural networks is teh learning rate, which is basically a measure on how tiny our nudge to the weights is going to be.In this particular case, I coded the learning_rate to decrease with every iteration as follows:learning_rate = 0.01 / (iteration + 1)this is important to ensure that once the weights are nearing the optimal values the adjustment in each iteration is subsequently more subtle.In the end, the perceptron always converges into a solution and finds with great precision the line we are looking for.Perceptrons are quite a revelation in that they can resolve equations by learning, however they are very limited. By their nature they can only resolve linear equations, so their problem space is quite narrow.Nowadays the neural networks consist of combinations of many perceptrons, in many layers, and other types of “neurons”, like convolution, recurrent, etc. increasing significantly the types of problems they solve."
    } 
  
]
